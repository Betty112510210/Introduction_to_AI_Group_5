# -*- coding: utf-8 -*-
"""youtube_api_scratch

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1egSPEO96C6T7Q8HUh4wB4IBlbflRafaw

時間排序
"""

import requests
import json

# 替換為你的 YouTube API 金鑰
API_KEY = 'AIzaSyAPk4HE1cfE-kQyrqYJG2cCuAynz7Iuiik'

# 目標影片 ID
VIDEO_ID = 'cdeKX7cs-r0'

# API 請求 URL
url = 'https://www.googleapis.com/youtube/v3/commentThreads'

# 請求參數
params = {
    'part': 'snippet',
    'videoId': VIDEO_ID,
    'maxResults': 5,       # 抓前 5 則留言
    'textFormat': 'plainText',
    'key': API_KEY
}

# 發送 GET 請求
response = requests.get(url, params=params)

# 處理回應
if response.status_code == 200:
    data = response.json()
    for idx, item in enumerate(data['items']):
        comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
        author = item['snippet']['topLevelComment']['snippet']['authorDisplayName']
        print(f'{idx + 1}. {author}: {comment}\n')
else:
    print(f'錯誤！HTTP 狀態碼：{response.status_code}')
    print(response.text)

import requests
import csv

# 替換成你的 YouTube API 金鑰
API_KEY = 'AIzaSyAPk4HE1cfE-kQyrqYJG2cCuAynz7Iuiik'
VIDEO_ID = 'cdeKX7cs-r0'

# 存在 Colab 當前目錄
file_path = "comments.csv"

url = 'https://www.googleapis.com/youtube/v3/commentThreads'
params = {
    'part': 'snippet',
    'videoId': VIDEO_ID,
    'maxResults': 5,
    'order': 'relevance',
    'textFormat': 'plainText',
    'key': API_KEY
}

response = requests.get(url, params=params)

if response.status_code == 200:
    data = response.json()
    with open(file_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['Author', 'Comment', 'LikeCount', 'PublishedAt'])
        for item in data['items']:
            snippet = item['snippet']['topLevelComment']['snippet']
            writer.writerow([
                snippet['authorDisplayName'],
                snippet['textDisplay'],
                snippet.get('likeCount', 0),
                snippet['publishedAt']
            ])
    print("✅ 已成功儲存 comments.csv")
else:
    print(f"❌ 錯誤 {response.status_code}: {response.text}")

"""2000筆，包括前五筆回覆，只要英文
Jennie-Coachella
"""

!pip install langdetect
import requests
import csv
from langdetect import detect
from google.colab import files

API_KEY = 'AIzaSyAPk4HE1cfE-kQyrqYJG2cCuAynz7Iuiik'  # ⚠️ 請填入你的 YouTube API 金鑰
VIDEO_ID = 'cdeKX7cs-r0'
MAX_COMMENTS = 2000
file_path = "english_comments_with_replies.csv"

# === 初始化 ===
comments = []
url = 'https://www.googleapis.com/youtube/v3/commentThreads'
params = {
    'part': 'snippet,replies',
    'videoId': VIDEO_ID,
    'maxResults': 100,
    'order': 'relevance',
    'textFormat': 'plainText',
    'key': API_KEY
}

# === 抓留言（含回覆）直到 2000 筆 ===
while len(comments) < MAX_COMMENTS:
    response = requests.get(url, params=params)
    if response.status_code != 200:
        print(f"❌ 錯誤 {response.status_code}: {response.text}")
        break

    data = response.json()
    for item in data['items']:
        top = item['snippet']['topLevelComment']['snippet']
        try:
            if detect(top['textDisplay']) == 'en':
                comments.append({
                    'Type': 'TopLevel',
                    'Author': top['authorDisplayName'],
                    'Comment': top['textDisplay'],
                    'LikeCount': top.get('likeCount', 0),
                    'PublishedAt': top['publishedAt']
                })
        except:
            continue  # 偵測失敗就跳過

        # 回覆
        if 'replies' in item:
            for reply in item['replies']['comments']:
                snippet = reply['snippet']
                try:
                    if detect(snippet['textDisplay']) == 'en':
                        comments.append({
                            'Type': 'Reply',
                            'Author': snippet['authorDisplayName'],
                            'Comment': snippet['textDisplay'],
                            'LikeCount': snippet.get('likeCount', 0),
                            'PublishedAt': snippet['publishedAt']
                        })
                except:
                    continue

    # 如果已經抓滿 2000 筆，跳出迴圈
    if len(comments) >= MAX_COMMENTS:
        break

    # 檢查是否還有下一頁
    if 'nextPageToken' not in data:
        break
    params['pageToken'] = data['nextPageToken']

# === 寫入 CSV ===
with open(file_path, 'w', newline='', encoding='utf-8') as f:
    writer = csv.DictWriter(f, fieldnames=['Type', 'Author', 'Comment', 'LikeCount', 'PublishedAt'])
    writer.writeheader()
    for row in comments[:MAX_COMMENTS]:
        writer.writerow(row)

print(f"✅ 共儲存 {len(comments)} 筆英文留言（含回覆）")

# === 下載檔案 ===
files.download(file_path)

"""抓包括所有回覆，只有英文，lesserafim coachella / gaza news_aljazerra"""

import requests
import csv
import re
import time

API_KEY = 'AIzaSyAPk4HE1cfE-kQyrqYJG2cCuAynz7Iuiik'  # ← 替換成你的 YouTube API 金鑰
VIDEO_ID = "gkEfGofiJuo" #'Fw4XXFhCEus'
COMMENT_THREAD_URL = 'https://www.googleapis.com/youtube/v3/commentThreads'
COMMENT_LIST_URL = 'https://www.googleapis.com/youtube/v3/comments'

def is_english(text):
    return bool(re.search(r'[a-zA-Z]', text))

def get_all_comment_threads():
    comments = []
    params = {
        'part': 'snippet',
        'videoId': VIDEO_ID,
        'key': API_KEY,
        'maxResults': 100,
        'order': 'relevance'
    }
    while True:
        response = requests.get(COMMENT_THREAD_URL, params=params)
        data = response.json()

        for item in data.get('items', []):
            snippet = item['snippet']['topLevelComment']['snippet']
            author = snippet.get('authorDisplayName')
            time_published = snippet.get('publishedAt')
            text = snippet.get('textDisplay')

            if is_english(text):
                comments.append([author, time_published, text, ""])

            # 如果這則留言有回覆
            total_replies = item['snippet'].get('totalReplyCount', 0)
            if total_replies > 0:
                parent_id = item['snippet']['topLevelComment']['id']
                replies = get_all_replies(parent_id)
                for r_author, r_time, r_text in replies:
                    comments.append([r_author, r_time, "", r_text])

        if 'nextPageToken' in data and len(comments) < 2000:
            params['pageToken'] = data['nextPageToken']
        else:
            break
    return comments[:2000]

def get_all_replies(parent_id):
    replies = []
    params = {
        'part': 'snippet',
        'parentId': parent_id,
        'key': API_KEY,
        'maxResults': 100
    }
    while True:
        response = requests.get(COMMENT_LIST_URL, params=params)
        data = response.json()

        for item in data.get('items', []):
            snippet = item['snippet']
            author = snippet.get('authorDisplayName')
            time_published = snippet.get('publishedAt')
            text = snippet.get('textDisplay')

            if is_english(text):
                replies.append((author, time_published, text))

        if 'nextPageToken' in data:
            params['pageToken'] = data['nextPageToken']
        else:
            break

        time.sleep(0.1)  # 避免 hitting rate limit
    return replies

# 取得留言與回覆
print("🚀 抓取中...")
comments = get_all_comment_threads()

# 寫入 CSV
filename = "youtube_english_comments_full.csv"
with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
    writer = csv.writer(f)
    writer.writerow(['Author', 'PublishedAt', 'Comment', 'Reply'])
    writer.writerows(comments)

print(f"✅ 完成！共 {len(comments)} 筆英文留言與回覆已儲存到 {filename}")

"""減少雜訊版本，前200則"""

!pip install google-api-python-client langdetect emoji

from googleapiclient.discovery import build
from langdetect import detect
import csv
import time
import re

# YouTube API 設定
api_key = 'AIzaSyAPk4HE1cfE-kQyrqYJG2cCuAynz7Iuiik'  # ← 請換成你的 API 金鑰
video_id = 'YSvMAV2Ni3s'
youtube = build('youtube', 'v3', developerKey=api_key)

# 處理留言內容：去除 <br> 與換行，但保留 emoji
def clean_comment(text):
    text = text.replace('<br>', ' ')
    text = text.replace('\n', ' ')
    return text.strip()

# 判斷是否為英文（保留 emoji，不刪除）
def is_english(text):
    try:
        # 只剔除換行標籤，保留 emoji 再判斷語言
        temp = clean_comment(text)
        return detect(temp) == 'en'
    except:
        return False

# 抓最多 200 則英文留言與回覆（保留 emoji）
def get_top_related_comments(video_id, max_results=200):
    results = []
    next_page_token = None

    while len(results) < max_results:
        response = youtube.commentThreads().list(
            part='snippet,replies',
            videoId=video_id,
            maxResults=100,
            order='relevance',
            textFormat='plainText',
            pageToken=next_page_token
        ).execute()

        for item in response['items']:
            # 主留言
            comment = item['snippet']['topLevelComment']['snippet']
            author = comment.get('authorDisplayName', '')
            text = comment['textDisplay']
            published_at = comment['publishedAt']

            if is_english(text):
                cleaned = clean_comment(text)
                results.append([author, cleaned, published_at, False])

            # 回覆
            if 'replies' in item:
                for reply in item['replies']['comments']:
                    r = reply['snippet']
                    reply_text = r['textDisplay']
                    reply_author = r.get('authorDisplayName', '')
                    reply_time = r['publishedAt']

                    if is_english(reply_text):
                        cleaned_reply = clean_comment(reply_text)
                        results.append([reply_author, cleaned_reply, reply_time, True])

            if len(results) >= max_results:
                break

        if len(results) >= max_results or 'nextPageToken' not in response:
            break

        next_page_token = response.get('nextPageToken')
        time.sleep(0.5)

    return results[:max_results]

# 取得資料
comments_data = get_top_related_comments(video_id, max_results=200)

# 存成 CSV（Colab 使用者可搭配下載）
filename = 'top_200_english_with_emoji.csv'
with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
    writer = csv.writer(f)
    writer.writerow(['Author', 'Comment', 'PublishedAt', 'IsReply'])
    writer.writerows(comments_data)

print(f'✔ 已儲存 {len(comments_data)} 則留言至 {filename}')