# -*- coding: utf-8 -*-
"""youtube_api_scratch

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1egSPEO96C6T7Q8HUh4wB4IBlbflRafaw

æ™‚é–“æ’åº
"""

import requests
import json

# æ›¿æ›ç‚ºä½ çš„ YouTube API é‡‘é‘°
API_KEY = 'AIzaSyAPk4HE1cfE-kQyrqYJG2cCuAynz7Iuiik'

# ç›®æ¨™å½±ç‰‡ ID
VIDEO_ID = 'cdeKX7cs-r0'

# API è«‹æ±‚ URL
url = 'https://www.googleapis.com/youtube/v3/commentThreads'

# è«‹æ±‚åƒæ•¸
params = {
    'part': 'snippet',
    'videoId': VIDEO_ID,
    'maxResults': 5,       # æŠ“å‰ 5 å‰‡ç•™è¨€
    'textFormat': 'plainText',
    'key': API_KEY
}

# ç™¼é€ GET è«‹æ±‚
response = requests.get(url, params=params)

# è™•ç†å›æ‡‰
if response.status_code == 200:
    data = response.json()
    for idx, item in enumerate(data['items']):
        comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
        author = item['snippet']['topLevelComment']['snippet']['authorDisplayName']
        print(f'{idx + 1}. {author}: {comment}\n')
else:
    print(f'éŒ¯èª¤ï¼HTTP ç‹€æ…‹ç¢¼ï¼š{response.status_code}')
    print(response.text)

import requests
import csv

# æ›¿æ›æˆä½ çš„ YouTube API é‡‘é‘°
API_KEY = 'AIzaSyAPk4HE1cfE-kQyrqYJG2cCuAynz7Iuiik'
VIDEO_ID = 'cdeKX7cs-r0'

# å­˜åœ¨ Colab ç•¶å‰ç›®éŒ„
file_path = "comments.csv"

url = 'https://www.googleapis.com/youtube/v3/commentThreads'
params = {
    'part': 'snippet',
    'videoId': VIDEO_ID,
    'maxResults': 5,
    'order': 'relevance',
    'textFormat': 'plainText',
    'key': API_KEY
}

response = requests.get(url, params=params)

if response.status_code == 200:
    data = response.json()
    with open(file_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['Author', 'Comment', 'LikeCount', 'PublishedAt'])
        for item in data['items']:
            snippet = item['snippet']['topLevelComment']['snippet']
            writer.writerow([
                snippet['authorDisplayName'],
                snippet['textDisplay'],
                snippet.get('likeCount', 0),
                snippet['publishedAt']
            ])
    print("âœ… å·²æˆåŠŸå„²å­˜ comments.csv")
else:
    print(f"âŒ éŒ¯èª¤ {response.status_code}: {response.text}")

"""2000ç­†ï¼ŒåŒ…æ‹¬å‰äº”ç­†å›è¦†ï¼Œåªè¦è‹±æ–‡
Jennie-Coachella
"""

!pip install langdetect
import requests
import csv
from langdetect import detect
from google.colab import files

API_KEY = 'AIzaSyAPk4HE1cfE-kQyrqYJG2cCuAynz7Iuiik'  # âš ï¸ è«‹å¡«å…¥ä½ çš„ YouTube API é‡‘é‘°
VIDEO_ID = 'cdeKX7cs-r0'
MAX_COMMENTS = 2000
file_path = "english_comments_with_replies.csv"

# === åˆå§‹åŒ– ===
comments = []
url = 'https://www.googleapis.com/youtube/v3/commentThreads'
params = {
    'part': 'snippet,replies',
    'videoId': VIDEO_ID,
    'maxResults': 100,
    'order': 'relevance',
    'textFormat': 'plainText',
    'key': API_KEY
}

# === æŠ“ç•™è¨€ï¼ˆå«å›è¦†ï¼‰ç›´åˆ° 2000 ç­† ===
while len(comments) < MAX_COMMENTS:
    response = requests.get(url, params=params)
    if response.status_code != 200:
        print(f"âŒ éŒ¯èª¤ {response.status_code}: {response.text}")
        break

    data = response.json()
    for item in data['items']:
        top = item['snippet']['topLevelComment']['snippet']
        try:
            if detect(top['textDisplay']) == 'en':
                comments.append({
                    'Type': 'TopLevel',
                    'Author': top['authorDisplayName'],
                    'Comment': top['textDisplay'],
                    'LikeCount': top.get('likeCount', 0),
                    'PublishedAt': top['publishedAt']
                })
        except:
            continue  # åµæ¸¬å¤±æ•—å°±è·³é

        # å›è¦†
        if 'replies' in item:
            for reply in item['replies']['comments']:
                snippet = reply['snippet']
                try:
                    if detect(snippet['textDisplay']) == 'en':
                        comments.append({
                            'Type': 'Reply',
                            'Author': snippet['authorDisplayName'],
                            'Comment': snippet['textDisplay'],
                            'LikeCount': snippet.get('likeCount', 0),
                            'PublishedAt': snippet['publishedAt']
                        })
                except:
                    continue

    # å¦‚æœå·²ç¶“æŠ“æ»¿ 2000 ç­†ï¼Œè·³å‡ºè¿´åœˆ
    if len(comments) >= MAX_COMMENTS:
        break

    # æª¢æŸ¥æ˜¯å¦é‚„æœ‰ä¸‹ä¸€é 
    if 'nextPageToken' not in data:
        break
    params['pageToken'] = data['nextPageToken']

# === å¯«å…¥ CSV ===
with open(file_path, 'w', newline='', encoding='utf-8') as f:
    writer = csv.DictWriter(f, fieldnames=['Type', 'Author', 'Comment', 'LikeCount', 'PublishedAt'])
    writer.writeheader()
    for row in comments[:MAX_COMMENTS]:
        writer.writerow(row)

print(f"âœ… å…±å„²å­˜ {len(comments)} ç­†è‹±æ–‡ç•™è¨€ï¼ˆå«å›è¦†ï¼‰")

# === ä¸‹è¼‰æª”æ¡ˆ ===
files.download(file_path)

"""æŠ“åŒ…æ‹¬æ‰€æœ‰å›è¦†ï¼Œåªæœ‰è‹±æ–‡ï¼Œlesserafim coachella / gaza news_aljazerra"""

import requests
import csv
import re
import time

API_KEY = 'AIzaSyAPk4HE1cfE-kQyrqYJG2cCuAynz7Iuiik'  # â† æ›¿æ›æˆä½ çš„ YouTube API é‡‘é‘°
VIDEO_ID = "gkEfGofiJuo" #'Fw4XXFhCEus'
COMMENT_THREAD_URL = 'https://www.googleapis.com/youtube/v3/commentThreads'
COMMENT_LIST_URL = 'https://www.googleapis.com/youtube/v3/comments'

def is_english(text):
    return bool(re.search(r'[a-zA-Z]', text))

def get_all_comment_threads():
    comments = []
    params = {
        'part': 'snippet',
        'videoId': VIDEO_ID,
        'key': API_KEY,
        'maxResults': 100,
        'order': 'relevance'
    }
    while True:
        response = requests.get(COMMENT_THREAD_URL, params=params)
        data = response.json()

        for item in data.get('items', []):
            snippet = item['snippet']['topLevelComment']['snippet']
            author = snippet.get('authorDisplayName')
            time_published = snippet.get('publishedAt')
            text = snippet.get('textDisplay')

            if is_english(text):
                comments.append([author, time_published, text, ""])

            # å¦‚æœé€™å‰‡ç•™è¨€æœ‰å›è¦†
            total_replies = item['snippet'].get('totalReplyCount', 0)
            if total_replies > 0:
                parent_id = item['snippet']['topLevelComment']['id']
                replies = get_all_replies(parent_id)
                for r_author, r_time, r_text in replies:
                    comments.append([r_author, r_time, "", r_text])

        if 'nextPageToken' in data and len(comments) < 2000:
            params['pageToken'] = data['nextPageToken']
        else:
            break
    return comments[:2000]

def get_all_replies(parent_id):
    replies = []
    params = {
        'part': 'snippet',
        'parentId': parent_id,
        'key': API_KEY,
        'maxResults': 100
    }
    while True:
        response = requests.get(COMMENT_LIST_URL, params=params)
        data = response.json()

        for item in data.get('items', []):
            snippet = item['snippet']
            author = snippet.get('authorDisplayName')
            time_published = snippet.get('publishedAt')
            text = snippet.get('textDisplay')

            if is_english(text):
                replies.append((author, time_published, text))

        if 'nextPageToken' in data:
            params['pageToken'] = data['nextPageToken']
        else:
            break

        time.sleep(0.1)  # é¿å… hitting rate limit
    return replies

# å–å¾—ç•™è¨€èˆ‡å›è¦†
print("ğŸš€ æŠ“å–ä¸­...")
comments = get_all_comment_threads()

# å¯«å…¥ CSV
filename = "youtube_english_comments_full.csv"
with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
    writer = csv.writer(f)
    writer.writerow(['Author', 'PublishedAt', 'Comment', 'Reply'])
    writer.writerows(comments)

print(f"âœ… å®Œæˆï¼å…± {len(comments)} ç­†è‹±æ–‡ç•™è¨€èˆ‡å›è¦†å·²å„²å­˜åˆ° {filename}")

"""æ¸›å°‘é›œè¨Šç‰ˆæœ¬ï¼Œå‰200å‰‡"""

!pip install google-api-python-client langdetect emoji

from googleapiclient.discovery import build
from langdetect import detect
import csv
import time
import re

# YouTube API è¨­å®š
api_key = 'AIzaSyAPk4HE1cfE-kQyrqYJG2cCuAynz7Iuiik'  # â† è«‹æ›æˆä½ çš„ API é‡‘é‘°
video_id = 'YSvMAV2Ni3s'
youtube = build('youtube', 'v3', developerKey=api_key)

# è™•ç†ç•™è¨€å…§å®¹ï¼šå»é™¤ <br> èˆ‡æ›è¡Œï¼Œä½†ä¿ç•™ emoji
def clean_comment(text):
    text = text.replace('<br>', ' ')
    text = text.replace('\n', ' ')
    return text.strip()

# åˆ¤æ–·æ˜¯å¦ç‚ºè‹±æ–‡ï¼ˆä¿ç•™ emojiï¼Œä¸åˆªé™¤ï¼‰
def is_english(text):
    try:
        # åªå‰”é™¤æ›è¡Œæ¨™ç±¤ï¼Œä¿ç•™ emoji å†åˆ¤æ–·èªè¨€
        temp = clean_comment(text)
        return detect(temp) == 'en'
    except:
        return False

# æŠ“æœ€å¤š 200 å‰‡è‹±æ–‡ç•™è¨€èˆ‡å›è¦†ï¼ˆä¿ç•™ emojiï¼‰
def get_top_related_comments(video_id, max_results=200):
    results = []
    next_page_token = None

    while len(results) < max_results:
        response = youtube.commentThreads().list(
            part='snippet,replies',
            videoId=video_id,
            maxResults=100,
            order='relevance',
            textFormat='plainText',
            pageToken=next_page_token
        ).execute()

        for item in response['items']:
            # ä¸»ç•™è¨€
            comment = item['snippet']['topLevelComment']['snippet']
            author = comment.get('authorDisplayName', '')
            text = comment['textDisplay']
            published_at = comment['publishedAt']

            if is_english(text):
                cleaned = clean_comment(text)
                results.append([author, cleaned, published_at, False])

            # å›è¦†
            if 'replies' in item:
                for reply in item['replies']['comments']:
                    r = reply['snippet']
                    reply_text = r['textDisplay']
                    reply_author = r.get('authorDisplayName', '')
                    reply_time = r['publishedAt']

                    if is_english(reply_text):
                        cleaned_reply = clean_comment(reply_text)
                        results.append([reply_author, cleaned_reply, reply_time, True])

            if len(results) >= max_results:
                break

        if len(results) >= max_results or 'nextPageToken' not in response:
            break

        next_page_token = response.get('nextPageToken')
        time.sleep(0.5)

    return results[:max_results]

# å–å¾—è³‡æ–™
comments_data = get_top_related_comments(video_id, max_results=200)

# å­˜æˆ CSVï¼ˆColab ä½¿ç”¨è€…å¯æ­é…ä¸‹è¼‰ï¼‰
filename = 'top_200_english_with_emoji.csv'
with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
    writer = csv.writer(f)
    writer.writerow(['Author', 'Comment', 'PublishedAt', 'IsReply'])
    writer.writerows(comments_data)

print(f'âœ” å·²å„²å­˜ {len(comments_data)} å‰‡ç•™è¨€è‡³ {filename}')